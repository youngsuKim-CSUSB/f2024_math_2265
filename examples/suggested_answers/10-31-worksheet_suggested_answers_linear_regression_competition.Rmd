---
title: "Linear Regression Basics"
author: "your name"
date: "2024-10-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(openintro)
library(ggplot2)
if (!tinytex::is_tinytex()) {
  latex_installed <- Sys.which("tlmgr") != ""
  
  if (!latex_installed) {
    tinytex::install_tinytex()
  } else {
    message("A LaTeX distribution already exists on the system.")
  }
} # this ensures TinyTeX is installed only if no LaTeX distribution is found
library(tinytex)  # to knit to PDF
set.seed(2265)
```

## Math 2265 Chapter 8. Linear Regression

- Work as a group!
- You will need to replace `"ans"` or `your_answer` in the source code
- Update your name in L3
- Add your group members' name below; students may lose one point if Question 0 is unanswered
- Make sure you save and `knit` your work (to html or pdf) before submitting it to Canvas
- Please only submit your work if you attended the class and worked with other students; this is not an online course

---

### Goal

1. Inference using a  linear model
1. Apply linear regression to a real work data set

---

### Question 0. Who are your group members? (List their first names)

**Answer:** 

  1. `<name_1>`
  1. `<name_2>`

---

### If you need more time to get used to `Markdown`, use the `Visual` mode.

The icon is located in the upper-left corner next to `source`. 

---

### `duke_forest`: Sale prices of houses in Duke Forest, Durham, NC

Data Set: `duke_forest`.

We will work with the `duke_forest` data set.
Write a script to display the number of observations and the names of the variables of the data set. 

```{r}
# write your code 
str(duke_forest)
``` 

Recall we use linear regression for a pair of numerical variables, explanatory (often denoted by x) and response variable (often denoted by y).   

Save all the numerical variables as an R vector to the R variable `numerical_variables`. 

- Just copy the names one by one and paste them.
- Variable names need to be enclosed with quotation marks "variable_name"

```{r}
# numerical_variable <- c("variable_name1","variable_name2",...,...)
numerical_variable <- c("price","bed","bath","area","year_built","lot")
numerical_variable 
```

Among the variables we will use `price` as the response variable. 
The goal is to find the $$most suitable explanatory variable$$ for linear regression.
The word most requires justification, and we will use two methods: 

1. The correlation coefficient $R$. 
2. The sum of squared residuals with respect to the least squares line.

### Best with respect to the correlation coefficient

To do this, we will need to compute the correlation coefficients over all the numerical variables you found above. Use the following to compute the correlation coefficients. 

```{r}
### 
#cor_1 <- 
#cor_2 <- 
#cor_3 <-
#cor_4 <-
#cor_5 <-
#cor_6 <-
#cor_7 <-
#cor_8 <-
# use as many as you need and delete unused ones

cor_1 <- cor(x=duke_forest$price, y=duke_forest$bed)
cor_2 <- cor(x=duke_forest$price, y=duke_forest$bath)
cor_3 <- cor(x=duke_forest$price, y=duke_forest$area)
cor_4 <- cor(x=duke_forest$price, y=duke_forest$year_built)
cor_5 <- cor(x=duke_forest$price, y=duke_forest$lot)
  
# cor_vec <- c(cor_1, cor_2, cor_3, your_answer, your_answer, ... ) 
cor_vec <- c(cor_1, cor_2, cor_3, cor_4, cor_5)
cor_vec
```

One of the results may be "NA" since that variable has missing value(s). 

Use `sort`, `max`, or `min` to determine the best one. Recall when determining the best, we need to consider their absolute values. 

```{r}
sort(cor_vec)
```

### Best with respect to the least squares line

To do this, we will eventually need to use `lm`. However, to use linear regression, we need to check that the data follows the linear trend. In other words, we need to check their scatter plot. 

In the next cell, make scatter plots for each of the explanatory variables. 

```{r}
ggplot2(data=duke_forest, aes(x=price,y=your_naswer))+
  geom_point()

ggplot2(data=duke_forest, aes(x=price,y=your_naswer))+
  geom_point()

ggplot2(data=duke_forest, aes(x=price,y=your_naswer))+
  geom_point()

# Use as many as you need and delete the rest
ggplot2(data=duke_forest, aes(x=price,y=your_naswer))+
  geom_point()

ggplot2(data=duke_forest, aes(x=price,y=your_naswer))+
  geom_point()

ggplot2(data=duke_forest, aes(x=price,y=your_naswer))+
  geom_point()
```

Use the correlation coefficient and scatter plots to choose two best candidates for linear regression. Now we will use `lm` to find the sum of squared residuals for those two cases.

```{r}
model1 <- lm( price ~ your_answer, data=duke_forest)
summary(model1) 
```

Compute the sum of the squared residuals and save it to `ssr1`.

```{r}
ssr1 <- sum( your_answer) 
ssr1
```

```{r}
model2 <- lm( price ~ your_answer, data=duke_forest)
summary(model2) 
```

Compute the sum of the squared residuals and save it to `ssr2`.

```{r}
ssr2 <- sum( your_answer) 
ssr2
```

### Make prediction based on your model

First plot the line with the data.

```{r}
ggplot2(data=duke_forest, aes(x = your_answer, y = price)) +
  geom_point() + 
  geom_abline(intercept = your_answer, slope = your_answer)
```

Make a prediction of the 50th observation in the variable:

```{r}
x_50 <- your_answer[50]

y_hat <- function(x) {
  intercept + slope * x
}

print(paste(c("y: ", price[50], "y_hat: ", yhat(x_50))))
```

## Please only submit your work if you attended the class and worked with other students; this is not an online course

### Share your work and help your group members before uploading your work to Canvas
