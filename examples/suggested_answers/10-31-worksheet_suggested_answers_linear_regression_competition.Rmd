---
title: "Linear Regression II"
author: "your name"
date: "2024-10-31"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(openintro)
library(ggplot2)
if (!tinytex::is_tinytex()) {
  latex_installed <- Sys.which("tlmgr") != ""
  
  if (!latex_installed) {
    tinytex::install_tinytex()
  } else {
    message("A LaTeX distribution already exists on the system.")
  }
} # this ensures TinyTeX is installed only if no LaTeX distribution is found
library(tinytex)  # to knit to PDF
set.seed(2265)
```

## Math 2265 Chapter 8. Linear Regression

- Work as a group!
- You will need to replace `"ans"` or `your_answer` in the source code
- Update your name in L3
- Add your group members' name below; students may lose one point if Question 0 is unanswered
- Make sure you save and `knit` your work (to html or pdf) before submitting it to Canvas
- Please only submit your work if you attended the class and worked with other students; this is not an online course

---

### Goal

1. Inference using a  linear model
1. Apply linear regression to a real work data set

---

### Question 0. Who are your group members? (List their first names)

**Answer:** 

  1. `<name_1>`
  1. `<name_2>`

---

### If you need more time to get used to `Markdown`, use the `Visual` mode.

The icon is located in the upper-left corner next to `source`. 

---

### `duke_forest`: Sale prices of houses in Duke Forest, Durham, NC

Data Set: `duke_forest`.

We will work with the `duke_forest` data set.
Write a script to check the number of observations and the names of the variables of the data set. 

```{r}
# write your code 
``` 

Recall we use linear regression for a pair of numerical variables, explanatory (horizontal, often denoted by x) and response variable (vertical, often denoted by y).   


### Task 

Save all the numerical variables as an R vector to the R variable `numerical_variables` below 

- Just copy the names from the previous output and paste them as parameters of the vector 
- Strings (e.g., variable names) need to be enclosed with quotation marks "variable_name"

```{r}
numerical_variable <- c("variable_name1","variable_name2",...,...)
numerical_variable 
```

Among the variables we will use `price` as the response variable. 
The goal is to find the **most suitable explanatory variable** for linear regression.
The word *most* requires justification, and we will use two crietria: 

1. The correlation coefficient $R$. 
2. The sum of squared residuals with respect to the least squares line.

### Best with respect to the correlation coefficient

To do this, we will need to compute the correlation coefficients of `price` over all the rest of the numerical variables you found above. Use the following block to compute the correlation coefficients. 

```{r}
### save results into variable cor_n
cor_1 <- 
cor_2 <- 
cor_3 <- 
# add additional cor_ as many as you need

cor_vec <- c(cor_1, cor_2, cor_3, your_answer, your_answer, ... ) 
cor_vec
```

Use `sort`, `max`, or `min` to determine the best one. Recall when determining the best, we need to consider their absolute values. 

```{r}
sort(abs(cor_vec))
```

### Question: Which variables are linearly associated `price`? 

Choose top two:

1. your_answer
2. your_answer

### Best with respect to the least squares line

To do this, we will use `lm`. However, to use linear regression, we need to check that the data follows the linear trend. That is, the association is linear. We can check this using their scatter plots. 

In the next cell, make scatter plots for each of the explanatory variables. 

```{r}
ggplot2(data=duke_forest, aes(x=your_naswer,y=price))+
  geom_point()

ggplot2(data=duke_forest, aes(x=your_naswer,y=price))+
  geom_point()

ggplot2(data=duke_forest, aes(x=your_naswer,y=price))+
  geom_point()

# Use as many as you need and delete the rest
ggplot2(data=duke_forest, aes(x=your_naswer,y=price))+
  geom_point()

ggplot2(data=duke_forest, aes(x=your_naswer,y=price))+
  geom_point()

ggplot2(data=duke_forest, aes(x=your_naswer,y=price))+
  geom_point()
```

Use the correlation coefficient and scatter plots to choose two best candidates for linear regression. Now we will use `lm` to find the least suqres line and the sum of squared residuals for each case. 

Save the linear model for your 1st choice

```{r}
model1 <- lm( price ~ your_answer, data=duke_forest)
summary(model1) 
```

Compute the sum of the squared residuals and save it to `ssr1`.

```{r}
ssr1 <- sum( your_answer) 
ssr1
```

Save the linear model for your 2nd choice

```{r}
model2 <- lm( price ~ your_answer, data=duke_forest)
summary(model2) 
```

Compute the sum of the squared residuals and save it to `ssr2`.

```{r}
ssr2 <- sum( your_answer) 
ssr2
```

### Question:

Which variable explains the responds variable `price` better?

### Make prediction based on your model

First plot the least squares line. Use your answer above as your explanatory variable. 

```{r}
ggplot2(data=duke_forest, aes(x = your_answer, y = price)) +
  geom_point() + 
  geom_abline(intercept = your_answer, slope = your_answer)
```

Make a prediction where your $x$ value is the median and mean of your variable. That is if your variable is called `my_expanatory_variable`, $x = median(my_expanatory_variable)$ or $x = mean(my_expanatory_variable)$.

```{r}
x <- median(my_expanatory_variable)
# or use mean (or do both)
# x <- mean(my_expanatory_variable)

y_hat <- function(x) {
  intercept + slope * x
}

print(paste(c("For", x, "y_hat(x) = ", yhat(x))))
```

## Please only submit your work if you attended the class and worked with other students; this is not an online course

## Complete the handout as a group, and turn it in to the instructor

<!-- ### Share your work and help your group members before uploading your work to Canvas -->
