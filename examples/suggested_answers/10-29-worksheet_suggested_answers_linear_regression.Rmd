---
title: "Linear Regression Basics"
author: "your name"
date: "2024-10-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(openintro)
library(ggplot2)
if (!tinytex::is_tinytex()) {
  latex_installed <- Sys.which("tlmgr") != ""
  
  if (!latex_installed) {
    tinytex::install_tinytex()
  } else {
    message("A LaTeX distribution already exists on the system.")
  }
} # this ensures TinyTeX is installed only if no LaTeX distribution is found
library(tinytex)  # to knit to PDF
set.seed(2265)
```

This will download the dataset needed for this worksheet. If it does not work, notify the instructor.

```{r, include=FALSE}
# Run this to download the scatter plot and line image
url <- "https://github.com/youngsuKim-CSUSB/f2024_math_2265/raw/linear_regression/examples/data/correlated_datasets.csv"
destfile <- "correlated_datasets.csv"  
download.file(url, destfile, mode = "wb")
```

## Math 2265 Chapter 8. Linear Regression

- Work as a group!
- You will need to replace `"ans"` or `your_answer` in the source code
- Update your name in L3
- Add your group members' name below; students may lose one point if Question 0 is unanswered
- Make sure you save and `knit` your work (to html or pdf) before submitting it to Canvas

---

### Goal

- Review the basic concepts in linear regression
  - Meaning of line fitting
    - Residuals
  - The $R$-value
  - how to use an `R`-function to find the least squares line 

---

### Question 0. Who are your group members? (List their first names)

**Answer:** 

  1. `<name_1>`
  1. `<name_2>`

---

### If you need more time to get used to `Markdown`, use the `Visual` mode.

The icon is located in the upper-left corner next to `source`. 

---

### Correlation factor R

The following snippet loads four data sets into data frames `df1, df2, df3, df4` (We do not need to understand the code as it is a bit involved). Each data frame consisting of two variables $x$ and $y$. The variable $x$ is shared among the data frames. 

You may need to install the package `gridExtra`. Save the file and check the pop-up notification at the top of this edit box.

```{r}
library(gridExtra)

# Read data from the CSV file
df <- read.csv("correlated_datasets.csv")

# We randomly choose 4 sets
target_correlations <- sample(c(2:9),4)

# Loop over the selected column positions
counter <- 1
for (i in target_correlations) {
  var_name <- paste0("df", counter)
  assign(var_name, data.frame(x = df$x, y = df[, i]))
  counter <- counter + 1
}


plots <- list()
# Loop through the four data frames (df1, df2, df3, and df4)
for (i in 1:4) {
  # Dynamically get each data frame by name
  df_name <- paste0("df", i)
  data <- get(df_name)
  
  # Create the scatter plot
  p <- ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.6) +
    labs(title = df_name) +
    coord_fixed() +  # Maintain a 1:1 aspect ratio
    theme_minimal()
  
  plots[[i]] <- p
}

# Arrange all four plots in a single row
grid.arrange(grobs = plots, ncol = 4)
```

### Question

Without computing the R-values, list the data frames in the increasing order of their R-values.

### Answer: "your_answer: list df1, df2, df3, df4 accordingly" 

---

### Computing the R-values

Here is an example of computing the R-value of `df1`. Recall `$` is used to grab a variable (column) by name.

```{r}
cor_df1 <- cor(x = df1$x, y = df1$y)
cor_df1
```

### Task 1

Compute the other R-values to confirm your answer.

```{r}
cor_df2 <- cor(x = df2$x, y = df2$y)
cor_df3 <- cor(x = df3$x, y = df3$y)
cor_df4 <- cor(x = df4$x, y = df4$y)
# display all four R values
c(cor_df1, cor_df2, cor_df3, cor_df4)
```

### Linear Regression (best fit line)

We will use the function `lm` to find the least squares line (the best fitting line): Use the first column for the intercept and slope and sketch its graph. Here is an example for `df1`.

```{r}
model <- lm(y~x, data=df1)
summary(model); print(paste("R-squared from cor:", cor_df1^2))
```

```{r}
ggplot(data=df1, aes(x=x,y=y)) +
  geom_point() +
  # you may also copy the numbers in the first column instead of using coef(model)[n]
  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color='hotpink')
```

#### The sum of square residuals

We will compute the sum of square residuals and save it to the variable `my_ssr`. We learned a way in the last worksheet. The `lm` function provides a convenient way. 

```{r} 
# Computes the residuals of the least squares line
residuals(model)
```

It is a vector, so we need to take the sum of their squares with a couple of commands

```{r} 
ssr <- sum( (residuals(model))^2 )
ssr
```


---

### Task 3

Find the best fitting line for `df2`, sketch the scatter plot and the line, and compute the sum of the square residuals.

```{r}
# linear model
model2 <- lm(y~x, data = df2)
summary(model2)
```

```{r}
ggplot(data=df2, aes(x=x,y=y)) +
  geom_point() +
  geom_abline(intercept = coef(model2)[1], slope = coef(model2)[2], color='hotpink')
```

```{r} 
# write your code below


my_ssr2 <- your_answer
```

#### Plot residuals

We will use `df3`. First save the residuals of `df3` as a variable 

```{r}
model3 <- lm(y~x, data=df3)
df3$res <- residuals(model3)
```

```{r}
hist(df3$res)
```

Does it follow normal distributions?

```{r}
ggplot(data=df3, mapping = aes(x = x, y = res)) +
  geom_point(color='blue')
```


### Share your work and help your group members before uploading your work to Canvas
