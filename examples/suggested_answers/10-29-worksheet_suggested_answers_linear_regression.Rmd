---
title: "Linear Regression I"
author: "your name"
date: "2024-10-29"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(openintro)
library(ggplot2)
if (!tinytex::is_tinytex()) {
  latex_installed <- Sys.which("tlmgr") != ""
  
  if (!latex_installed) {
    tinytex::install_tinytex()
  } else {
    message("A LaTeX distribution already exists on the system.")
  }
} # this ensures TinyTeX is installed only if no LaTeX distribution is found
library(tinytex)  # to knit to PDF
set.seed(2265)
```

This will download the dataset needed for this worksheet. If it does not work, notify the instructor.

```{r, include=FALSE}
# Run this to download the scatter plot and line image
url <- "https://github.com/youngsuKim-CSUSB/f2024_math_2265/raw/linear_regression/examples/data/correlated_datasets.csv"
destfile <- "correlated_datasets.csv"  
download.file(url, destfile, mode = "wb")
```

## Math 2265 Chapter 8. Linear Regression

- Work as a group!
- You will need to replace `"ans"` or `your_answer` in the source code
- Update your name in L3
- Add your group members' name below; students may lose one point if Question 0 is unanswered
- Make sure you save and `knit` your work (to html or pdf) before submitting it to Canvas

---

### Goal

- Review the basic concepts in linear regression
- Meaning of line fitting
  - Residuals
- The $R$-value
- how to use the `lm` function in R to find the least squares line 

---

### Question 0. Who are your group members? (List their first names)

**Answer:** 

  1. `<name_1>`
  1. `<name_2>`

---

### If you need more time to get used to `Markdown`, use the `Visual` mode.

The icon is located in the upper-left corner next to `source`. 

---

### Correlation Coefficient R

The following snippet loads four data sets into data frames `df1, df2, df3, df4` (we do not need to understand the code as it is a bit involved). Each data frame consists of two variables, $x$ and $y$, where the $x$ variable is the same across all data frames.

You may need to install the package `gridExtra` to display the plots side-by-side. Save the file and check the pop-up notification at the top of this edit box.

```{r}
set.seed(0)
library(gridExtra)

# Read data from the CSV file
df <- read.csv("correlated_datasets.csv")

# We randomly choose 4 sets
target_correlations <- sample(c(2:9),4)

# Loop over the selected column positions
counter <- 1
for (i in target_correlations) {
  var_name <- paste0("df", counter)
  assign(var_name, data.frame(x = df$x, y = df[, i]))
  counter <- counter + 1
}


plots <- list()
# Loop through the four data frames (df1, df2, df3, and df4)
for (i in 1:4) {
  # Dynamically get each data frame by name
  df_name <- paste0("df", i)
  data <- get(df_name)
  
  # Create the scatter plot
  p <- ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.6) +
    labs(title = df_name) +
    coord_fixed() +  # Maintain a 1:1 aspect ratio
    theme_minimal()
  
  plots[[i]] <- p
}

# Arrange all four plots in a single row
grid.arrange(grobs = plots, ncol = 4)
```

### Question

Without computing the correlation coefficients (R-values), list the data frames in increasing order.

```{r}
# change the order of 1,2,3,4 accordingly
my_sorted_cor_coef <- c(1,2,3,4)
my_sorted_cor_coef
```

---

### Computing Correlation Coefficients with `cor`

Here is an example of computing the correlation coefficient of `df1`. Recall `$` is used to grab a variable (column) by name.

```{r}
cor_df1 <- cor(x = df1$x, y = df1$y)
cor_df1
```

### Task 1

Compute the other correlation coefficients (R-values) to confirm your answer.

```{r}
cor_df2 <- cor(x = df2$x, y = df2$y)
cor_df3 <- cor(x = df3$x, y = df3$y)
cor_df4 <- cor(x = df4$x, y = df4$y)
# display all correlation coefficients
c(cor_df1, cor_df2, cor_df3, cor_df4)
```

Check your answer to the question above. 

### Linear Regression (best fit line)

We will use the function `lm` to find the least squares line (the best fitting line): Use the first column for the intercept and slope and sketch its graph. Here is an example for `df1`.

```{r}
model <- lm(y~x, data=df1)
summary(model); print(paste("R-squared from cor:", cor_df1^2))
```

```{r}
ggplot(data=df1, aes(x=x,y=y)) +
  geom_point() +
  # you may also copy the numbers in the first column instead of using coef(model)[n]
  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color='hotpink')
```

#### The sum of squared residuals

We will compute the sum of squared residuals and save it to the variable `my_ssr`. We learned a way in the last worksheet. The `lm` function provides a convenient way. 

```{r} 
# Computes the residuals of the least squares line
residuals(model)
```

It is a vector, so we need to take the sum of their squares with a couple of commands

```{r} 
ssr <- sum( (residuals(model))^2 )
ssr
```

---

### Task 2

Find the least squares line for `df2`, sketch the scatter plot and the line, and compute the sum of the squared residuals.

Least squares line

```{r}
# linear model
model2 <- lm(y~x, data = df2)
summary(model2)
```

Scatter plot

```{r}
ggplot(data=df2, aes(x=x,y=y)) +
  geom_point() +
  geom_abline(intercept = coef(model2)[1], slope = coef(model2)[2], color='hotpink')
```

Sum of the squared residuals

```{r} 
# write your code below
my_ssr2 <- sum( residuals(model2)^2 )
my_ssr2
```


### Plot residuals

### Task 3

First find the least squares line and save the residuals of `df4` as a variable 

```{r}
model4 <- lm(y~x, data=df4)
df4$res <- residuals(model4)
```

Add the intercept and slope of `model4`.

```{r}
ggplot(data=df4, aes(x=x,y=y)) +
  geom_point() +
  geom_abline(intercept = coef(model4)[1], slope = coef(model4)[2], color='hotpink')
```

This histogram explains the term `nearly normal residuals`. That is the resudials follow a normal distribution.

```{r}
hist(df4$res)
```

This is the scatter plot of the residuals. 

```{r}
ggplot(data=df4, mapping = aes(x = x, y = res)) +
  geom_point(color='blue') + 
  geom_abline(intercept = 0, slope = 0)
```


### Check your answers

TRUE: correct
FALSE: incorrect

```{r}
my_sorted_cor_coef == c(3,2,1,4)
```


```{r}
my_ssr2 == sum( residuals(model2)^2 )  # 
```


### Share your work and help your group members before uploading your work to Canvas
